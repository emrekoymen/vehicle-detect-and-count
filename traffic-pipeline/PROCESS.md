# Project Process and Execution

## 1. Dataset Exploration and Understanding

**Task (from README):** Explore the dataset, understand the structure, labeling conventions, and class diversity.

**Action:**
Your dataset is in YOLO format under `datasets/yolo/` and uses `traffic-pipeline/dataset.yaml` for configuration.

*   **To convert your YOLO dataset to COCO format (for Faster R-CNN):**
    The `prepare-data.py` script has been updated to read your `dataset.yaml` and your YOLO data to produce a COCO JSON file.
    ```bash
    python traffic-pipeline/prepare-data.py --yaml_path traffic-pipeline/dataset.yaml --output_coco_path traffic-pipeline/datasets/yolo_coco_from_yolo.json
    ```
    This will output the COCO annotation file to:
    -   `traffic-pipeline/datasets/yolo_coco_from_yolo.json`

*   **To explore the dataset:**
    -   Manually inspect your directories: `datasets/yolo/images/{train,val}/`, `datasets/yolo/labels/{train,val}/`.
    -   Review `traffic-pipeline/dataset.yaml`. Ensure its `path` key correctly points to `../datasets/yolo` (relative to `dataset.yaml` itself) and that `train`, `val` keys point to the correct subdirectories (e.g., `images/train`, `images/val`). Ensure class names (`names:`) are accurate.
    -   After conversion, inspect the generated `traffic-pipeline/datasets/yolo_coco_from_yolo.json`.

## 2. Image Preprocessing

**Task (from README):** Preprocess the images for model input: resize, normalize, augment with noise or occlusion to simulate real traffic conditions.

**Action:**
*   **Initial Preprocessing (Resize, Normalize - for COCO conversion):**
    The `traffic-pipeline/prepare-data.py` script (when converting YOLO to COCO) reads image dimensions and converts YOLO's relative bounding box coordinates to COCO's absolute format. YOLO itself handles image resizing/normalization at training time based on `imgsz` and internal transformations.

*   **Data Augmentation (Noise, Occlusion, etc.):**
    Data augmentation is typically integrated into the training pipelines:
    -   **For YOLOv8:** Augmentations are managed by the Ultralytics training engine, configured via hyperparameters in `traffic-pipeline/train-yolov8.py` (e.g., passed to `model.train()`) or through settings in `traffic-pipeline/dataset.yaml` if supported by Ultralytics for specific augmentations.
        ```bash
        # Training script incorporates augmentations based on its internal settings or config
        python traffic-pipeline/train-yolov8.py --data traffic-pipeline/dataset.yaml --epochs 50 --batch 32 # (other params as needed, e.g., --device cuda)
        ```
    -   **For Faster R-CNN:** Augmentations can be defined in the `CocoDet` dataset class (in `traffic-pipeline/train-fasterrcnn.py`) by modifying the transform pipeline. The current version uses basic normalization. To add specific augmentations like noise or occlusion, you would need to modify `train-fasterrcnn.py` to include relevant torchvision transform operations.
        ```bash
        # Training script incorporates augmentations if defined in its data loading
        python traffic-pipeline/train-fasterrcnn.py --coco_json traffic-pipeline/datasets/yolo_coco_from_yolo.json --epochs 50 # (other params as needed)
        ```

## 3. Model Training

**Task (from README):** Train two object detectors: Faster R-CNN and YOLOv8 (or latest YOLO version), using pretrained weights as a starting point. Train for at least 50 epochs and include validation loss monitoring. Apply regularization or data augmentation if overfitting is detected. (Also covers README Task 7 aspects).

**Action:**
*   **Train YOLOv8:**
    Uses `traffic-pipeline/dataset.yaml` (which should point to `../datasets/yolo`). Output in `traffic-pipeline/runs/yolov8/`.
    ```bash
    python traffic-pipeline/train-yolov8.py --data traffic-pipeline/dataset.yaml --model yolov8m.pt --epochs 50 --batch 32 --imgsz 640 # (e.g., --device cuda)
    ```
    (Adjust epochs, batch size, model type, device, etc. as needed using script arguments. Validation and logging are handled by Ultralytics.)

*   **Train Faster R-CNN:**
    Uses the COCO JSON generated by `prepare-data.py`. Output in `traffic-pipeline/runs/fasterrcnn/`.
    ```bash
    python traffic-pipeline/train-fasterrcnn.py --coco_json traffic-pipeline/datasets/yolo_coco_from_yolo.json --num_classes 5 --epochs 50 --batch_size 4
    ```
    (Adjust epochs, batch size, learning rate, num_classes (your object classes + 1 background) as needed. Validation is performed after each epoch if a validation set is found.)

**Note on Overfitting & Task 7:**
-   Monitor validation metrics. For YOLOv8, see TensorBoard logs in `traffic-pipeline/runs/yolov8/`. For Faster R-CNN, monitor console output from `evaluate()`.
-   Adjust learning rate, weight decay, add more data augmentation (see Task 2), or use early stopping (YOLOv8 has `patience` param) if overfitting is observed.

## 4. Object Tracking Implementation

**Task (from README):** Implement object tracking across frames using DeepSORT or ByteTrack to enable counting.

**Action:**
The project uses ByteTrack.
*   **Base Tracking Demonstration (YOLOv8 + ByteTrack):**
    The `track_bytetrack.py` script (renamed from `track-bytetrack.py`) shows detection and tracking.
    ```bash
    python traffic-pipeline/track_bytetrack.py --weights traffic-pipeline/runs/yolov8/weights/best.pt --source path/to/your/video.mp4
    ```
    (Use your trained YOLOv8 weights and video source.)

*   **Integrated Application with Detection, Tracking, and Counting:**
    The `realtime-app.py` script is the most comprehensive solution.
    ```bash
    python traffic-pipeline/realtime-app.py --weights traffic-pipeline/runs/yolov8/weights/best.pt --source path/to/your/video.mp4
    ```
    (Logs counts to a CSV in `traffic-pipeline/logs/`.)

*   **Note on `count-vehicle.py`:**
    This script is now primarily **conceptual**. It shows how one *might* add counting logic to a tracker like `track_bytetrack.py`. It imports from the renamed `track_bytetrack.py`. For a fully working counting application, use `realtime-app.py`.
    ```bash
    python traffic-pipeline/count-vehicle.py 
    ```
    (This will run a demonstration with simulated counts as the script itself does not integrate live counting into the tracker directly.)

## 5. Model Evaluation

**Task (from README):** Evaluate detection accuracy using metrics like precision, recall, IoU, and mAP. Assess tracking with metrics like MOTA (Multiple Object Tracking Accuracy).

**Action:**
*   **Detection Accuracy (Precision, Recall, mAP):**
    The `evaluate-detectors.py` script evaluates both models and saves metrics to `traffic-pipeline/reports/detection_metrics.csv` by default.
    ```bash
    python traffic-pipeline/evaluate-detectors.py --yolo_weights traffic-pipeline/runs/yolov8/weights/best.pt --dataset_yaml traffic-pipeline/dataset.yaml --fasterrcnn_weights traffic-pipeline/runs/fasterrcnn/model_e49.pth --coco_json_val traffic-pipeline/datasets/yolo_coco_from_yolo.json
    ```
    (Adjust paths to your best model weights. The script expects `dataset.yaml` for YOLOv8 and the COCO JSON for Faster R-CNN. Ensure the `CocoDet` class in `train_fasterrcnn.py` can provide a `val_ds.coco` COCO API object for its validation subset for full Faster R-CNN evaluation; otherwise, it will be skipped with a warning.)

*   **Tracking Accuracy (MOTA):**
    As before, this requires external tools (e.g., `py-motmetrics`). Modify `realtime-app.py` to save tracking results in a standard format (e.g., MOTChallenge format) for evaluation against ground truth tracking annotations.

## 6. Real-Time Application Integration

**Task (from README):** Integrate everything into a real-time video processing application that detects, tracks, and counts vehicles from webcam or traffic camera footage.

**Action:**
The `realtime-app.py` script serves this purpose.
    ```bash
    # For webcam input:
    python traffic-pipeline/realtime-app.py --weights traffic-pipeline/runs/yolov8/weights/best.pt --source 0 # (e.g., --device cuda)

    # For a video file input:
    python traffic-pipeline/realtime-app.py --weights traffic-pipeline/runs/yolov8/weights/best.pt --source path/to/your/video.mp4 # (e.g., --device cuda)
    ```

## 7. Training Duration and Monitoring (Covered by Task 3)

**Task (from README):** Train for at least 50 epochs and include validation loss monitoring. Apply regularization or data augmentation if overfitting is detected.

**Action:** See notes under Task 3: Model Training. 